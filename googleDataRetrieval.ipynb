{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5a62db",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7835c54",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Data consists of items (scientific articles, books and citations) from Google Scholar searches on equine colic. There are about 1000 items. The collected information contains the title, the truncated abstract or description and the metainfo (which contains the authors, publication journal or website for articles, the publisher for books, and the year of publication).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b8faf",
   "metadata": {},
   "source": [
    "## Workspace and Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c07dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the information for each item\n",
    "def get_articles_metadata(soup):\n",
    "    '''\n",
    "    Returns lists with articles information.\n",
    "    INPUT:\n",
    "        soup (bs4.BeautifulSoup) \n",
    "    OUTPUT:\n",
    "        titles, abstracts, augumented_titles, metainfo (lists)\n",
    "    '''\n",
    "    \n",
    "    titles, abstracts, metainfo = [], [], []\n",
    "    \n",
    "    # Get the item's title\n",
    "    article_tags = soup.select('[data-lid]')\n",
    "    for entry in article_tags:\n",
    "        titles.append(entry.select('h3')[0].get_text())\n",
    "    \n",
    "    # Get the metainfo: authors, publication, year\n",
    "    authors_tags = soup.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "    for entry in authors_tags:\n",
    "        metainfo.append(entry.get_text())\n",
    "    \n",
    "    # Get the truncated abstract or description\n",
    "    abstract_tags = soup.find_all(\"div\", {\"class\": \"gs_rs\"})\n",
    "    for entry in abstract_tags:\n",
    "        abstracts.append(entry.get_text())\n",
    "        \n",
    "    # Combine titles and truncated abstract \n",
    "    augumented_titles = list(zip(titles, abstracts))\n",
    "        \n",
    "    return titles, abstracts, augumented_titles, metainfo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the items repository\n",
    "articles_repos_dict = {'Title' : [], 'TruncAbstr' : [], \n",
    "                       'AugmTitle' : [], 'Info' : []}\n",
    "\n",
    "# Add records to the repository\n",
    "def add_to_repo(titles, abstracts, augmented_titles, metainfo):\n",
    "    articles_repos_dict['Title'].extend(titles)\n",
    "    articles_repos_dict['TruncAbstr'].extend(abstracts)\n",
    "    articles_repos_dict['AugmTitle'].extend(augmented_titles)\n",
    "    articles_repos_dict['Info'].extend(metainfo)\n",
    "    \n",
    "    return pd.DataFrame(articles_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb24832",
   "metadata": {},
   "source": [
    "## Scrape Google Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b1a13",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Some of the data was collected and parsed via crawling the Google Scholar page.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400e8a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a requests session \n",
    "\n",
    "s = requests.Session()\n",
    "s.auth = ('user', 'pass')\n",
    "s.headers.update({'x-test': 'true'})\n",
    "\n",
    "# both 'x-test' and 'x-test2' are sent\n",
    "s.get('http://httpbin.org/headers', headers={'x-test2': 'true'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9627bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the information from one page (contains 10 items)\n",
    "\n",
    "def get_page_info(url):\n",
    "    \n",
    "    '''\n",
    "    Downloads a page with a given url.\n",
    "    \n",
    "    INPUT:\n",
    "        url (str) - specify the url for the page to download\n",
    "        \n",
    "    OUTPUT:\n",
    "        soup (bs4 object) - Beautiful Soup object\n",
    "    '''\n",
    "    \n",
    "    # Download the page \n",
    "    response = s.get(url)\n",
    "    \n",
    "    # Check if the response is succesful \n",
    "    if response.status_code != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    raise Exception('Failed to fetch web page. ')\n",
    "        \n",
    "    # Parse using beautiful soup\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from several pages\n",
    "# Must be slow not to overload the servers\n",
    "\n",
    "url_equine = 'https://scholar.google.com/scholar?start={}&q=\"equine+colic\"+&hl=en&as_sdt=0,5'\n",
    "\n",
    "# Number of articles to extract, there are 10 articles per page\n",
    "n = 100\n",
    "\n",
    "for i in range (0,40,10):\n",
    "    \n",
    "    # Get url for the each page\n",
    "    url = url_equine.format(i)\n",
    "    #url = url_human.format(i)\n",
    "    \n",
    "    # Get the soup object for each page\n",
    "    soup = get_page_info(url)\n",
    "    \n",
    "    # Collect all the information \n",
    "    titles, abstracts, augmented_titles, metainfo = get_articles_metadata(soup)\n",
    "    \n",
    "    # Add records to the repository\n",
    "    collection = add_to_repo(titles, abstracts, augmented_titles, metainfo)\n",
    "    \n",
    "    # Save the record in a file\n",
    "    collection.to_csv(f'data/eqcol_{i}.csv', index=False)\n",
    "  \n",
    "    # Use sleep to avoid status code 429\n",
    "    time.sleep(np.random.randint(30, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148ff65",
   "metadata": {},
   "source": [
    "## Parse Downloaded Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae8aed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Part of the data was collected by downloading the files separately and then parsing them with Beautiful Soup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe595f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the working directory and the file directories\n",
    "directory_human = os.getcwd()+'/files_human'\n",
    "directory_horse = os.getcwd()+'/files_horse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a04b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse each html file with BeautifulSoup\n",
    "def get_local_data(directory, file_to_save):\n",
    "    \n",
    "    # Loop through all the files present in the given directory\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        # Check the file extension\n",
    "        if filename.endswith('.html'):\n",
    "          \n",
    "            # Get the full path of the file\n",
    "            fname = os.path.join(directory, filename)\n",
    "          \n",
    "            # Open the file and create a BeautifulSoup object\n",
    "            with open(fname, 'r') as file:\n",
    "                soup = BeautifulSoup(file.read(), 'lxml')\n",
    "            \n",
    "                # Collect all the neccessary information \n",
    "                titles, abstracts, augmented_titles, metainfo = get_articles_metadata(soup)\n",
    "            \n",
    "                # Add the records to the repository\n",
    "                collection = add_to_repo(titles, abstracts, augmented_titles, metainfo)\n",
    "                \n",
    "        # Save the records in a file\n",
    "        collection.to_csv(f'data/'+file_to_save+f'.csv', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d6e8d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Have to clean cache and rebuild the soup object for each group of files, otherwise the soup object will add on the top of the previous step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the files on equine colic\n",
    "horse_local_data= get_local_data(directory_horse, 'horse_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "horse_check = pd.read_csv('data/horse_local.csv')\n",
    "horse_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60e275",
   "metadata": {},
   "source": [
    "## Combine the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ace8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data obtained via crawling the Google Scholar\n",
    "eqcol_df = pd.read_csv('data/eqcol_460.csv')\n",
    "eqcol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes on equine colic\n",
    "horse_full = pd.concat([eqcol_df, horse_check], ignore_index=True, sort=False)\n",
    "horse_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataframe to a file\n",
    "horse_full.to_csv(f'data/horse_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b09545",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Nandini Saini, Scraping Information of Research Papers on Google Scholar using Python, Medium (Aug 18, 2021)](https://medium.com/@nandinisaini021/scraping-publications-of-aerial-image-research-papers-on-google-scholar-using-python-a0dee9744728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c27cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
